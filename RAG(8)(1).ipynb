{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b6313d-c7cf-45cb-92df-e341fd3a63b5",
   "metadata": {},
   "source": [
    "Run the cells in this section to install the packages needed by the notebooks in this workshop.\n",
    "\n",
    "IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108c611c-7246-45c4-9f1e-76888b5076eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3>=1.28.57\n",
      "  Using cached boto3-1.42.11-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting awscli>=1.29.57\n",
      "  Using cached awscli-1.44.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore>=1.31.57\n",
      "  Using cached botocore-1.42.11-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.57)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3>=1.28.57)\n",
      "  Using cached s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.31.57)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore>=1.31.57)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore>=1.31.57)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting docutils<=0.19,>=0.18.1 (from awscli>=1.29.57)\n",
      "  Using cached docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting PyYAML<6.1,>=3.10 (from awscli>=1.29.57)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting colorama<0.4.7,>=0.2.5 (from awscli>=1.29.57)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.29.57)\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.29.57)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached boto3-1.42.11-py3-none-any.whl (140 kB)\n",
      "Using cached botocore-1.42.11-py3-none-any.whl (14.5 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached awscli-1.44.1-py3-none-any.whl (4.6 MB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached docutils-0.19-py3-none-any.whl (570 kB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.6.2\n",
      "\u001b[2K    Uninstalling urllib3-2.6.2:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.6.2\n",
      "\u001b[2K  Attempting uninstall: six\n",
      "\u001b[2K    Found existing installation: six 1.17.0\n",
      "\u001b[2K    Uninstalling six-1.17.0:\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/13\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: PyYAML━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/13\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.3━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K  Attempting uninstall: pyasn1━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K    Found existing installation: pyasn1 0.6.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K    Uninstalling pyasn1-0.6.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K      Successfully uninstalled pyasn1-0.6.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [PyYAML]\n",
      "\u001b[2K  Attempting uninstall: jmespath[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K    Found existing installation: jmespath 1.0.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling jmespath-1.0.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled jmespath-1.0.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K  Attempting uninstall: docutils━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K    Found existing installation: docutils 0.19━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling docutils-0.19:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled docutils-0.19━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [pyasn1]\n",
      "\u001b[2K  Attempting uninstall: colorama╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [docutils]\n",
      "\u001b[2K    Found existing installation: colorama 0.4.6━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [docutils]\n",
      "\u001b[2K    Uninstalling colorama-0.4.6:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [docutils]\n",
      "\u001b[2K      Successfully uninstalled colorama-0.4.6━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [docutils]\n",
      "\u001b[2K  Attempting uninstall: rsam╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [docutils]\n",
      "\u001b[2K    Found existing installation: rsa 4.7.2m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]]\n",
      "\u001b[2K    Uninstalling rsa-4.7.2:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K      Successfully uninstalled rsa-4.7.2[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K  Attempting uninstall: botocore1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K    Found existing installation: botocore 1.42.11━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [rsa]\n",
      "\u001b[2K    Uninstalling botocore-1.42.11:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.42.1190m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: s3transfer━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: s3transfer 0.16.0━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling s3transfer-0.16.0:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled s3transfer-0.16.00m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: boto3━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [s3transfer]\n",
      "\u001b[2K    Found existing installation: boto3 1.42.110m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [s3transfer]\n",
      "\u001b[2K    Uninstalling boto3-1.42.11:━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [s3transfer]\n",
      "\u001b[2K      Successfully uninstalled boto3-1.42.11\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [s3transfer]\n",
      "\u001b[2K  Attempting uninstall: awscli━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [s3transfer]\n",
      "\u001b[2K    Found existing installation: awscli 1.44.10m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [awscli]\n",
      "\u001b[2K    Uninstalling awscli-1.44.1:━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [awscli]\n",
      "\u001b[2K      Successfully uninstalled awscli-1.44.1\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [awscli]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [awscli]12/13\u001b[0m [awscli]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "jupyter-ai 2.31.6 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "sagemaker-studio 1.1.1 requires pydynamodb>=0.7.4, which is not installed.\n",
      "aiobotocore 2.22.0 requires botocore<1.37.4,>=1.37.2, but you have botocore 1.42.11 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "sagemaker-studio-analytics-extension 0.2.2 requires sparkmagic==0.22.0, but you have sparkmagic 0.21.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.3 awscli-1.44.1 boto3-1.42.11 botocore-1.42.11 colorama-0.4.6 docutils-0.19 jmespath-1.0.1 pyasn1-0.6.1 python-dateutil-2.9.0.post0 rsa-4.7.2 s3transfer-0.16.0 six-1.17.0 urllib3-2.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9416e-bade-4b17-9fa1-fd213a6691f0",
   "metadata": {},
   "source": [
    "This notebook demonstrates invoking Bedrock models directly using the AWS SDK, but for later part of this notebook, you'll also need to install other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a834cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install  \\\n",
    "    \"langchain>=0.0.350\" \\\n",
    "    \"transformers>=4.24,<5\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0 \\\n",
    "    datasets==2.15.0 \\\n",
    "    numexpr==2.8.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298bf02a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Restart Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3175e6e6-fd7b-452e-8558-6b2e7983767d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50187eb7-d6bc-446f-af38-98d80a04da4c",
   "metadata": {},
   "source": [
    "The boto3 provides different clients for Amazon Bedrock to perform different actions. The actions for InvokeModel and InvokeModelWithResponseStream are supported by Amazon Bedrock Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    runtime = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49dab59f-a09f-4f39-adf4-f3a90f79bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ommunitylangchain'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain -communitylangchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48760e9-9bb1-43a1-8fd8-20de6324a321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_256/1718514090.py:6: LangChainDeprecationWarning: The class `Bedrock` was deprecated in LangChain 0.0.34 and will be removed in 1.0. An updated version of the class exists in the `langchain-aws package and should be used instead. To use it run `pip install -U `langchain-aws` and import as `from `langchain_aws import BedrockLLM``.\n",
      "  llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':300})\n",
      "/tmp/ipykernel_256/1718514090.py:7: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 1.0. An updated version of the class exists in the `langchain-aws package and should be used instead. To use it run `pip install -U `langchain-aws` and import as `from `langchain_aws import BedrockEmbeddings``.\n",
      "  bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n"
     ]
    }
   ],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.llms import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':300})\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d95b5-1904-45d8-9413-d449b15d95d7",
   "metadata": {},
   "source": [
    "We begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude for text generation and Amazon Titan for text embedding.\n",
    "\n",
    "Note: It is possible to choose other models available with Bedrock. You can replace the model_id as follows to change the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369afb0-bff4-4732-8115-6d61f1bf6caf",
   "metadata": {},
   "source": [
    "Let's first download some of the files to build our document store. In this example I am downloading the official paper for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152ebd9c-be88-4b9e-92e6-9d9302844760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://arxiv.org/pdf/2005.11401.pdf\"\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f2a0b-eaeb-4aed-83e8-f7ed2249e1ba",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of DirectoryLoader from PyPDF available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. \n",
    "Also the embeddings model has a limit of the length of input tokens limited to 8192 tokens, which roughly translates to ~32,000 characters. \n",
    "For the sake of this use-case we are creating chunks of roughly 2000 characters with an overlap of 200 characters using RecursiveCharacterTextSplitter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c15644-b797-4807-aba6-9683719ede95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-text-splitters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2d86ca-3299-4865-a429-9af842eedda6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454342d6-0147-4911-884e-6916366a4756",
   "metadata": {},
   "source": [
    "Lets review how many chunks and characters we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2111c537-86f4-4eaf-aaf3-286309e753ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 19 documents loaded is 3637 characters.\n",
      "After the split we have 48 documents more than the original 19.\n",
      "Average length among 48 documents (after split) is 1542 characters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "abs\n",
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829e9c7-91be-468b-a8f7-7f205556a774",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding would look like for the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b922608-8367-4767-ab8b-574e23ed1a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:  Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis†‡, Ethan Perez⋆,\n",
      "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
      "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
      "†Facebook AI Research;‡University College London;⋆New York University;\n",
      "plewis@fb.com\n",
      "Abstract\n",
      "Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
      "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
      "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
      "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
      "decisions and updating their world knowledge remain open research problems. Pre-\n",
      "trained models with a differentiable access mechanism to explicit non-parametric\n",
      "memory have so far been only investigated for extractive downstream tasks. We\n",
      "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
      "ory for language generation. We introduce RAG models where the parametric\n",
      "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
      "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
      "pare two RAG formulations, one which conditions on the same retrieved passages\n",
      "across the whole generated sequence, and another which can use different passages\n",
      "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
      "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
      "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
      "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
      "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
      "seq2seq baseline.\n",
      "1 Introduction\n",
      "Sample embedding of a document chunk:  [ 0.6484375   0.24707031 -0.16503906 ...  0.11279297  0.1484375\n",
      " -0.27148438]\n",
      "Size of the embedding:  (1536,)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    sample_embedding = np.array(bedrock_embeddings.embed_query(docs[1].page_content))\n",
    "    print(\"Sample chunk: \",docs[0].page_content)\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "    print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d49a2-df55-492a-a8e4-ec3c98e6468a",
   "metadata": {},
   "source": [
    "Following the similar pattern embeddings could be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "This can be easily done using FAISS implementation inside LangChain which takes input the embeddings model and the documents to create the entire vector store. \n",
    "Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. \n",
    "VectorStoreIndexWrapper helps us with that.\n",
    "\n",
    "⚠️⚠️⚠️ NOTE: it might take few minutes to run the following cell ⚠️⚠️⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae6e4e-27a7-4129-87eb-6e2677dab5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45410471-8cc4-472b-9d2b-3961e4ea78c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "#from langchain_community.indexes import VectorstoreIndexCreator\n",
    "#from langchain_community.vectorstores import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "#wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0607e-e85b-4a88-9803-d900d862ca5f",
   "metadata": {},
   "source": [
    "Now that we have our vector store in place, we can start asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3076a2cc-dde3-4c67-a622-8f278a6f521d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Explain Retrieval Augment Generation to a 6th grader\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c08c572-c926-400f-8510-0a798dea8c3b",
   "metadata": {},
   "source": [
    "Lets review the mebedding for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb296a69-1d7b-4929-84b4-57c21582a04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62890625, -0.41796875, -0.234375  , ...,  0.66015625,\n",
       "        0.375     , -0.03222656])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = vectorstore_faiss.embedding_function.embed_query(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8f3bd-4644-4b41-9ac7-0e49649b221d",
   "metadata": {},
   "source": [
    "We can use this embedding of the query to then fetch relevant documents. Now our query is represented as embeddings we can do a similarity search of our query against our data store providing us with the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798bc385-ba57-493c-858c-21784f88e2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "----\n",
      "## Document 1: Appendices for Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "A Implementation Details\n",
      "For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\n",
      "For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\n",
      "Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as\n",
      "we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\n",
      "we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\n",
      "and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\n",
      "Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\n",
      "B Human Evaluation\n",
      "Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\n",
      "and a worked example appear when clicking \"view tool guide\".\n",
      "Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\n",
      "which model corresponded to sentence A and sentence B was randomly selected for each example.\n",
      "Annotators were encouraged to research the topic using the internet, and were given detailed instruc-\n",
      "tions and worked examples in a full instructions tab. We included some gold sentences in order to\n",
      "assess the accuracy of the annotators. Two annotators did not perform well on these examples and\n",
      "their annotations were removed from the results.\n",
      "C Training setup Details\n",
      "We train all RAG models and BART baselines using Fairseq [ 45].2We train with mixed precision\n",
      "ﬂoating point arithmetic [ 40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\n",
      "training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\n",
      "with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100\n",
      "GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace.......\n",
      "---\n",
      "## Document 2: The\tDivine\n",
      "Comedy\t(x) qQuery\n",
      "Encoder\n",
      "q(x)\n",
      "MIPS p θGenerator  pθ\n",
      "(Parametric)\n",
      "Margin-\n",
      "alize\n",
      "This\t14th\tcentury\twork\n",
      "is\tdivided\tinto\t3\n",
      "sections:\t\"Inferno\",\n",
      "\"Purgatorio\"\t&\n",
      "\"Paradiso\"\t\t\t\t\t\t\t\t\t (y)End-to-End Backprop through q and  p θ\n",
      "Barack\tObama\twas\n",
      "born\tin\tHawaii. (x)\n",
      "Fact V eriﬁcation: Fact Querysupports \t(y)\n",
      "Question GenerationFact V eriﬁcation:\n",
      "Label GenerationDocument\n",
      "IndexDefine\t\"middle\tear\" (x)\n",
      "Question Answering:\n",
      "Question QueryThe\tmiddle\tear\tincludes\n",
      "the\ttympanic\tcavity\tand\n",
      "the\tthree\tossicles.\t\t (y)\n",
      "Question Answering:\n",
      "Answer GenerationRetriever pη\n",
      "(Non-Parametric)\n",
      "z 4\n",
      "z3\n",
      "z2\n",
      "z 1d(z)\n",
      "Jeopardy Question\n",
      "Generation:\n",
      "Answer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document\n",
      "Index ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use\n",
      "Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\n",
      "treatzas a latent variable and marginalize over seq2seq predictions given different documents.\n",
      "but have only explored open-domain extractive question answering. Here, we bring hybrid parametric\n",
      "and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\n",
      "We endow pre-trained, parametric-memory generation models with a non-parametric memory through\n",
      "a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\n",
      "We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\n",
      "non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\n",
      "retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\n",
      "retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\n",
      "the input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with\n",
      "the input to generate the output. We marginalize the latent documents with a top-K approximation,.......\n",
      "---\n",
      "## Document 3: Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\n",
      "with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\n",
      "pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\n",
      "than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\n",
      "BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\n",
      "the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\n",
      "speciﬁc by a large margin. Table 3 shows typical generations from each model.\n",
      "Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform\n",
      "best because it can generate responses that combine content from several documents. Figure 2 shows\n",
      "an example. When generating “Sun”, the posterior is high for document 2 which mentions “The\n",
      "Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\n",
      "generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\n",
      "This observation suggests that the generator can complete the titles without depending on speciﬁc\n",
      "documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\n",
      "ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\n",
      "Sun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\n",
      "Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\n",
      "BART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\n",
      "with \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\n",
      "how parametric and non-parametric memories work together —the non-parametric component helps\n",
      "to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\n",
      "4.4 Fact Veriﬁcation.......\n",
      "---\n",
      "## Document 4: Document 1 : his works are considered classics of American\n",
      "literature ... His wartime experiences formed the basis for his novel\n",
      "”A Farewell to Arms” (1929) ...\n",
      "Document 2 : ... artists of the 1920s ”Lost Generation” expatriate\n",
      "community. His debut novel, ”The Sun Also Rises” , was published\n",
      "in 1926.\n",
      "BOS”\n",
      "TheSunAlsoRises”isa\n",
      "novelbythis\n",
      "authorof”A\n",
      "Farewellto\n",
      "Arms”Doc 1\n",
      "Doc 2\n",
      "Doc 3\n",
      "Doc 4\n",
      "Doc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y−i)for each generated token for input “Hem-\n",
      "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
      "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
      "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
      "responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\n",
      "Task Input Model Generation\n",
      "MS-\n",
      "MARCOdeﬁne middle\n",
      "earBART?The middle ear is the part of the ear between the middle ear and the nose.\n",
      "RAG-T The middle ear is the portion of the ear internal to the eardrum.\n",
      "RAG-S The middle ear includes the tympanic cavity and the three ossicles.\n",
      "what currency\n",
      "needed in\n",
      "scotlandBART The currency needed in Scotland is Pound sterling.\n",
      "RAG-T Pound is the currency needed in Scotland.\n",
      "RAG-S The currency needed in Scotland is the pound sterling.\n",
      "Jeopardy\n",
      "Question\n",
      "Gener\n",
      "-ationWashingtonBART?This state has the largest number of counties in the U.S.\n",
      "RAG-T It’s the only U.S. state named for a U.S. president\n",
      "RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\n",
      "The Divine\n",
      "ComedyBART*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\n",
      "RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\n",
      "RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\n",
      "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35].......\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ffa3d-b8d5-4a47-8552-b8f52fa27d96",
   "metadata": {},
   "source": [
    "You have the possibility to use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM. This wrapper performs the following steps behind the scences:\n",
    "\n",
    "    Take the question as input\n",
    "    Create question embedding\n",
    "    Fetch relevant documents\n",
    "    Stuff the documents and the question into a prompt\n",
    "    Invoke the model with the prompt and generate the answer in a human readable manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32341c-3bb8-4093-b6a8-8d8c9540feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain langchain-community langchain-text-splitters boto3 faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d77032ba-dfd4-43ca-be21-ad1c73630e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from langchain_community.chains import RetrievalQA  # modern supported QA chain\n",
    "#from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "retriever = vectorstore_faiss.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f1e892a-9590-42fb-811a-a46b0f0d1811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      4\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mHuman: Use the following pieces of context to provide a concise answer to the question at the end. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt try to make up an answer.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.chains'"
     ]
    }
   ],
   "source": [
    "from langchain_core.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 4}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "query = \"Explain Retrieval Augment Generation to a 6th grader\"\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a8dd0-55a8-4ae3-8143-df8b19f1c974",
   "metadata": {},
   "source": [
    "Review the documents which became context for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c9f4b-264e-4649-a76f-f9487f02ebfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fec46-b609-4528-bddc-6daf0cee987d",
   "metadata": {},
   "source": [
    "Lets ask a question which cannot be answewred on the the bases of provided content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78fde3-a7bf-493d-afb9-aeb4eba5b3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 4}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "query = \"Discuss challanges of DevSecOps\"\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af970a2b-d7e2-4824-a302-653efda0b42d",
   "metadata": {},
   "source": [
    "You can also query the vector database and find the similarity score. the lower the score is, the better the result is. Read more about this https://python.langchain.com/docs/integrations/vectorstores/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff34f83-96d5-4e73-bb5e-9a5f862bfea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_with_score(\"Explain benefits of RAG\")\n",
    "relevant_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8571fcb-29ad-4a57-9250-db8897b2497b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_with_score(\"Discuss challenges of DecSecOps\")\n",
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88cb2c-1171-4566-93e3-3433d75df87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"what is RAG\"\n",
    "#query_embedding = vectorstore_faiss.embedding_function.embed_query(query)\n",
    "\n",
    "\n",
    "#docs_and_scores = vectorstore_faiss._similarity_search_with_relevance_scores(query)\n",
    "docs_and_scores = docs._similarity_search_with_relevance_scores(query)\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through the results to access documents and scores\n",
    "for doc, score in docs_and_scores:\n",
    "    print(f\"Document content: {doc.page_content}\")\n",
    "    print(f\"Score (Distance): {score}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6dce78-e12e-4216-86c6-98a939b80f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore_faiss.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc84f9e-ab27-4cb5-936c-5871f4e4d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9b1c247-0e27-4b31-b680-425c3f393dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector database created with COSINE distance configuration.\n",
      "\n",
      "Search results for query: 'What is RAG'\n",
      "\n",
      "----------------------------------------\n",
      "Relevance Score (0-1, higher is better): 258.9944\n",
      "Document snippet: Broader Impact\n",
      "This work offers several positive societal beneﬁts over previous work: the fact that it is more\n",
      "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\n",
      "with generations that are more factual, and offers more control and interpretability. RAG could be\n",
      "employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\n",
      "with a medical index and asking it open-domain questions on that topic, or by helping people be more\n",
      "effective at their jobs.\n",
      "With these advantages also come potential downsides: Wikipedia, or any potential external knowledge\n",
      "source, will probably never be entirely factual and completely devoid of bias. Since RAG can be\n",
      "employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\n",
      "to a lesser extent, including that it might be used to generate abuse, faked or misleading content in\n",
      "the news or on social media; to impersonate others; or to automate the production of spam/phishing\n",
      "content [ 54]. Advanced language models may also lead to the automation of various jobs in the\n",
      "coming decades [ 16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\n",
      "misleading content and automated spam/phishing.\n",
      "Acknowledgments\n",
      "The authors would like to thank the reviewers for their thoughtful and constructive feedback on this\n",
      "paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\n",
      "would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\n",
      "thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\n",
      "program.\n",
      "References\n",
      "[1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\n",
      "Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\n",
      "Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\n",
      "----------------------------------------\n",
      "Relevance Score (0-1, higher is better): 264.7880\n",
      "Document snippet: tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\n",
      "the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\n",
      "\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\n",
      "without expensive, specialized “salient span masking” pre-training [ 20]. It is worth noting that RAG’s\n",
      "retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\n",
      "and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\n",
      "encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\n",
      "re-ranker nor extractive reader is necessary for state-of-the-art performance.\n",
      "There are several advantages to generating answers even when it is possible to extract them. Docu-\n",
      "ments with clues about the answer but do not contain the answer verbatim can still contribute towards\n",
      "a correct answer being generated, which is not possible with standard extractive approaches, leading\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "db = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    distance_strategy=DistanceStrategy.COSINE # This line sets the configuration\n",
    ")\n",
    "\n",
    "print(\"FAISS vector database created with COSINE distance configuration.\")\n",
    "\n",
    "# 4. Example search with scores (scores will be 0-1, higher is better)\n",
    "query = \"What is RAG\"\n",
    "results_with_scores = db.similarity_search_with_score(query, k=2)\n",
    "\n",
    "print(f\"\\nSearch results for query: '{query}'\\n\")\n",
    "\n",
    "for document, score in results_with_scores:\n",
    "    print(\"-\" * 40)\n",
    "    # When using COSINE strategy, LangChain maps distance [0, 2] to a relevance score [0, 1]\n",
    "    print(f\"Relevance Score (0-1, higher is better): {score:.4f}\")\n",
    "    print(f\"Document snippet: {document.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b2f14e3-19c3-4057-b08a-1c17a6a10a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrockConverse\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\" # Use your region\n",
    ")\n",
    "\n",
    "# Initialize the Bedrock LLM (Example using Claude 3 Haiku)\n",
    "\n",
    "# Initialize the Bedrock LLM using ChatBedrockConverse (which uses Messages API)\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\", # The model ID that caused the error previously\n",
    "    client=bedrock_client,\n",
    "    max_tokens=512,        # <-- Pass max_tokens directly\n",
    "    temperature=0.1 \n",
    "    # Note: Use 'max_tokens' instead of 'max_tokens_to_sample' for new models/APIs\n",
    "    #model_kwargs={\"max_tokens\": 512, \"temperature\": 0.1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a33d312-754f-4de1-a562-2c4c8ac21f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking question: Who is elon musk?\n",
      "\n",
      "Response from Bedrock LLM:\n",
      "I don't see any information about Elon Musk in the provided context documents. The documents appear to be from a research paper about RAG (Retrieval-Augmented Generation) models and contain references to academic work, but they don't contain biographical information about Elon Musk.\n",
      "\n",
      "To answer your question about who Elon Musk is, I would need different source material that actually contains information about him. The current context focuses on machine learning research, specifically around question-answering systems and language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "# --- 3. Define the RAG Prompt Template (Optimized for Chat Models) ---\n",
    "\n",
    "# Chat models prefer system prompts and specific message roles.\n",
    "# Use MessagesPlaceholder for more complex chat history handling if needed,\n",
    "# but for basic RAG, we stick to system/human roles.\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant.\\n\\nContext: {context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Build and Run the RAG Chain ---\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"Who is elon musk?\"\n",
    "print(f\"Asking question: {question}\\n\")\n",
    "\n",
    "# Invoke the chain\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(\"Response from Bedrock LLM:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7be040-3df4-4b26-a9ad-2ddcb1d1a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
