FROM alpine:3.18

# Install required tools
RUN apk add --no-cache bash curl tar libc6-compat \
    && apk add --no-cache --virtual .build-deps libstdc++ binutils

# Install glibc
ENV GLIBC_VERSION=2.35-r1
RUN curl -Lo /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \
    && curl -Lo glibc.apk https://github.com/sgerrand/alpine-pkg-glibc/releases/download/${GLIBC_VERSION}/glibc-${GLIBC_VERSION}.apk \
    && apk add glibc.apk \
    && rm -f glibc.apk

# Set environment variables for ODBC
ENV ODBCINI=/etc/odbc.ini
ENV ODBCSYSINI=/etc/

# Download and extract Amazon Redshift ODBC driver (manual step required to obtain the RPM)
# Assume driver is included in build context
COPY AmazonRedshiftODBC-64-bit.x86_64.rpm /tmp/
RUN apk add --no-cache rpm2cpio cpio \
    && mkdir -p /opt/amazon/redshiftodbc \
    && cd /opt/amazon/redshiftodbc \
    && rpm2cpio /tmp/AmazonRedshiftODBC-64-bit.x86_64.rpm | cpio -idmv \
    && rm /tmp/AmazonRedshiftODBC-64-bit.x86_64.rpm

# Add ODBC configuration files
COPY odbc.ini /etc/odbc.ini
COPY odbcinst.ini /etc/odbcinst.ini

CMD ["bash"]





[Redshift]
Description=Amazon Redshift ODBC
Driver=Amazon Redshift ODBC Driver
Server=your-redshift-cluster.amazonaws.com
Database=your_db
UID=your_user
PWD=your_password
Port=5439



[Amazon Redshift ODBC Driver]
Description=Amazon Redshift ODBC Driver
Driver=/opt/amazon/redshiftodbc/lib/amazonredshiftodbc.so





FROM alpine:3.18

# Install dependencies and glibc
RUN apk add --no-cache bash curl ca-certificates libstdc++ \
 && curl -Lo /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \
 && curl -Lo glibc.apk https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.35-r1/glibc-2.35-r1.apk \
 && apk add --no-cache glibc.apk

# Add Amazon Redshift ODBC driver manually
# You need to download the RPM manually and extract it
# (For example: AmazonRedshiftODBC-64-bit-1.X.X.X.X.x86_64.rpm)

COPY AmazonRedshiftODBC-64-bit*.rpm /tmp/

RUN apk add --no-cache rpm2cpio cpio \
 && cd /opt \
 && rpm2cpio /tmp/AmazonRedshiftODBC-64-bit*.rpm | cpio -idmv

# Set up ODBC configuration
COPY odbc.ini /etc/odbc.ini
COPY odbcinst.ini /etc/odbcinst.ini
ENV ODBCSYSINI=/etc

CMD ["bash"]




FROM debian:bullseye-slim

RUN apt-get update && apt-get install -y \
    unixodbc \
    libodbc1 \
    odbcinst \
    curl \
    unzip \
    && rm -rf /var/lib/apt/lists/*

# Copy and install Redshift ODBC RPM
COPY AmazonRedshiftODBC-64-bit*.rpm /tmp/

RUN apt-get update && apt-get install -y rpm2cpio cpio \
 && mkdir /opt/redshift \
 && cd /opt/redshift \
 && rpm2cpio /tmp/AmazonRedshiftODBC-64-bit*.rpm | cpio -idmv

# Configure ODBC
COPY odbc.ini /etc/odbc.ini
COPY odbcinst.ini /etc/odbcinst.ini
ENV ODBCSYSINI=/etc

CMD ["bash"]
.


FROM alpine:3.18

RUN apk add --no-cache unixodbc-dev postgresql-dev

# Configure ODBC (psqlODBC uses libpq)
COPY odbc.ini /etc/odbc.ini
COPY odbcinst.ini /etc/odbcinst.ini
ENV ODBCSYSINI=/etc

CMD ["bash"]





[Amazon Redshift ODBC Driver]
Description=Amazon Redshift ODBC Driver
Driver=/opt/amazon/redshiftodbc/lib/64/libamazonredshiftodbc.so


[PostgreSQL]
Description=ODBC for PostgreSQL
Driver=/usr/lib/psqlodbcw.so





FROM amazonlinux:2

# Install dependencies
RUN yum -y update && \
    yum -y install \
    curl \
    tar \
    gzip \
    icu \
    krb5-libs \
    libcurl \
    openssl \
    zlib \
    && yum clean all

# Set .NET version and install path
ENV DOTNET_VERSION=8.0.0
ENV DOTNET_DOWNLOAD_URL=https://download.visualstudio.microsoft.com/download/pr/0a8279f6-e503-4c1a-bba8-3dfd385c8c9c/3cb5c21e87719318b254ca4b7b4b0d37/dotnet-runtime-8.0.0-linux-x64.tar.gz
ENV DOTNET_ROOT=/usr/share/dotnet
ENV PATH=$PATH:$DOTNET_ROOT

# Download and install .NET 8 runtime
RUN mkdir -p "$DOTNET_ROOT" && \
    curl -SL "$DOTNET_DOWNLOAD_URL" | tar -xz -C "$DOTNET_ROOT"

# Verify installation
RUN dotnet --info

# Optional: set default entrypoint
# ENTRYPOINT ["dotnet"]







import json
import pandas as pd
from pandas import json_normalize

# Load JSON file
with open('data.json') as f:
    data = json.load(f)

# Extract the specific node (e.g., 'employees')
employees = data['employees']

# Normalize and flatten
df = json_normalize(employees)

# Export to CSV
df.to_csv('employees.csv', index=False)






# üîê Security Policy

## üìò Overview

This repository contains code written in **TypeScript (Node.js)** and **C# (.NET)**. This policy outlines our approach to keeping it secure through secure development practices, CI/CD controls, dependency hygiene, and vulnerability reporting.

---

## üéØ Scope

This policy applies to:

- All application and infrastructure code in this repository
- Build scripts, CI/CD pipelines, and related artifacts
- Contributors, maintainers, and DevOps engineers

---

## ‚úÖ Secure Coding Practices

### For All Code

- Follow [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- Validate and sanitize all user inputs
- Avoid hardcoding secrets, credentials, or tokens
- Apply the **principle of least privilege** for all configuration and code

### TypeScript

- Use `eslint` with `eslint-plugin-security` to catch insecure patterns
- Use strict types and avoid `any` where possible
- Avoid unsafe usage of `eval()`, `Function()`, or dynamic imports
- Avoid directly accessing external input in DB or command-line operations

### C#

- Use input validation libraries like `FluentValidation`
- Protect against SQL injection with parameterized queries (e.g., `SqlCommand.Parameters`)
- Use `HttpClientFactory` for safe HTTP calls (avoid raw `HttpClient` reuse)
- Use strong typing and nullability features
- Avoid reflection and unsafe code blocks unless justified

---

## üîê Secrets Management

- Do not store secrets, credentials, or tokens in source files
- Use `.gitignore` to exclude:
  - `.env`, `appsettings.Development.json`, `secrets.json`
- Secrets should be managed via:
  - Environment variables
  - GitHub/GitLab CI/CD secrets
  - Vault services (e.g., Azure Key Vault, AWS Secrets Manager)

Tools used:
- GitLab Secret Detection
- GitGuardian (optional)
- Pre-commit hooks with `detect-secrets`

---

## üì¶ Dependency Management

### TypeScript (Node.js)

- All packages must be declared in `package.json`
- Run `npm audit` or integrate [GitLab Dependency Scanning](https://docs.gitlab.com/ee/user/application_security/dependency_scanning/)
- Detect unused/missing packages using `depcheck`
- Detect unmaintained packages via custom scripts or `npm-check-updates`

### C# (.NET)

- All dependencies must be declared in `.csproj` or `packages.config`
- Use `dotnet list package --vulnerable` or [OWASP Dependency-Check](https://jeremylong.github.io/DependencyCheck/)
- Pin versions and avoid transitive bloat
- Use only packages with active support and security posture

---

## üß™ Static Application Security Testing (SAST)

- Enable SAST scans in CI for both languages:
  - TypeScript: ESLint + GitLab/GitHub SAST
  - C#: Roslyn analyzers, SonarQube, or GitLab SAST
- Critical/High severity issues must block merge unless a documented exception exists

---

## üß™ Additional Scans

- **Secret detection** is required in CI for every merge request
- **Container scanning** (if Dockerized) must run on all tagged builds
- **DAST** scans are optional but encouraged for exposed web services

---

## üõ°Ô∏è CI/CD Security Controls

- CI pipelines must run:
  - SAST
  - Dependency scanning
  - Secret scanning
- Pipelines must fail if:
  - Secrets are committed
  - Critical/High vulnerabilities are detected
- Build containers must be pulled from trusted registries and scanned

---

## üîç Code Review & Branch Protection

- Require at least **one reviewer** for all MRs/PRs
- Reviewers must validate:
  - Input sanitization
  - Secure use of third-party libraries
  - No secrets or sensitive keys in changes
- Enforce branch protections:
  - No force pushes to `main`, `release/*`
  - Require status checks to pass before merge

---

## üîê Repository Access Control

- Enforce 2FA for all contributors
- Use least privilege for repo permissions (Developer vs Maintainer roles)
- Rotate personal access tokens and CI/CD credentials regularly
- Audit repository access quarterly

---

## üö® Reporting a Vulnerability

If you discover a security vulnerability in this project, report it confidentially:

- Email: `security@example.com`
- Do **not** open public issues for vulnerabilities

We will:

- Acknowledge your report within 72 hours
- Provide a fix or mitigation within 14 days (target SLA)

---

## üìÖ Maintenance

- All dependencies must be reviewed for security updates monthly
- Security tools and CI scripts are updated quarterly
- This policy is reviewed and updated every 6 months

---

## üôè Acknowledgements

- OWASP Foundation
- GitLab Secure / GitHub Security
- Node.js Security WG
- Microsoft Secure Dev Practices

---
üõ† Step-by-Step OpenSSL Commands
üîπ Step 1: Generate Root CA

mkdir -p certs/root
cd certs/root

# Generate Root CA private key
openssl genrsa -out ca.key 4096

# Generate Root CA self-signed certificate (valid 10 years)
openssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 \
  -out ca.crt -subj "/C=US/ST=CA/O=MyRootCA/CN=MyRootCA"

üî∏ Step 2: Generate Intermediate CA

cd ../intermediate

# Generate Intermediate CA private key
openssl genrsa -out ca.key 4096

# Generate CSR for Intermediate CA
openssl req -new -key ca.key -out ca.csr -subj "/C=US/ST=CA/O=MyIntermediateCA/CN=MyIntermediateCA"

# Sign intermediate CSR with Root CA
openssl x509 -req -in ca.csr -CA ../root/ca.crt -CAkey ../root/ca.key -CAcreateserial \
  -out ca.crt -days 1825 -sha256

üîπ Step 3: Generate and Sign Client Certificate

cd ../clients

# Generate private key for client
openssl genrsa -out MyClient.key 2048

# Create CSR for client cert
openssl req -new -key MyClient.key -out MyClient.csr -subj "/CN=MyClient"

# Sign client CSR with Intermediate CA
openssl x509 -req -in MyClient.csr \
  -CA ../intermediate/ca.crt -CAkey ../intermediate/ca.key -CAcreateserial \
  -out MyClient.crt -days 365 -sha256

üîê Step 4: Bundle into PFX (PKCS#12)

# Combine full chain: Intermediate + Root
cat ../intermediate/ca.crt ../root/ca.crt > full_chain.crt

# Export client cert, private key, and chain to PFX
openssl pkcs12 -export \
  -out MyClient.pfx \
  -inkey MyClient.key \
  -in MyClient.crt \
  -certfile full_chain.crt \
  -name "MyClient"


var certCollection = new X509Certificate2Collection();
        certCollection.Import(pfxPath, password, X509KeyStorageFlags.Exportable);

        // Find the end-entity certificate (the one with private key)
        X509Certificate2 leafCert = null;
        foreach (var cert in certCollection)
        {
            if (cert.HasPrivateKey)
            {
                leafCert = cert;
                break;
            }
        }

        if (leafCert == null)
        {
            Console.WriteLine("No certificate with private key found in the PFX.");
            return;
        }

        // Build the chain
        var chain = new X509Chain();
        chain.ChainPolicy.ExtraStore.AddRange(certCollection);
        chain.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;

        bool isValid = chain.Build(leafCert);
        Console.WriteLine($"Chain is valid: {isValid}");

        // Print the chain
        Console.WriteLine("Chain elements:");
        foreach (var element in chain.ChainElements)
        {
            Console.WriteLine($"Subject: {element.Certificate.Subject}");
            Console.WriteLine($"Issuer : {element.Certificate.Issuer}");

            if (element.Certificate.Subject == element.Certificate.Issuer)
            {
                Console.WriteLine("This is a ROOT CA.");
            }

            Console.WriteLine();
        }




public static List<X509Certificate2> LoadPemCertificates(string pemPath)
    {
        string pemContent = File.ReadAllText(pemPath);
        var certificates = new List<X509Certificate2>();

        var matches = Regex.Matches(pemContent, "-----BEGIN CERTIFICATE-----(.*?)-----END CERTIFICATE-----", RegexOptions.Singleline);

        foreach (Match match in matches)
        {
            string base64 = match.Groups[1].Value.Replace("\r", "").Replace("\n", "");
            byte[] certBytes = Convert.FromBase64String(base64);
            var cert = new X509Certificate2(certBytes);
            certificates.Add(cert);
        }

        return certificates;
    }



    Reduce noise from automated security tools.

    Improve accuracy of vulnerability triaging.

    Close the loop between security and development teams.

    Drive continuous improvement in secure coding practices.

üóìÔ∏è How to Conduct the Meeting
1. Preparation (1 week before meeting)

    Security team aggregates all false positives flagged in the last quarter (from SAST, DAST, Dependency Scanning, etc.).

    Development team reviews assigned issues and marks what they believe are false positives, optionally with justification.

    Identify any tooling or rule misconfigurations leading to repeat issues.

    Prioritize high-severity or high-frequency false positives for discussion.

2. Attendees

    Security Champion(s)

    Tech Leads / Senior Engineers

    Software Engineers (rotating representatives if needed)

    DevSecOps or AppSec engineer

    Product Owner (optional if mitigation timelines affect delivery)

üìã Agenda of the Meeting
Time	Topic
0‚Äì10 mins	Overview: Trends in security findings and tool performance
10‚Äì30 mins	Review Top False Positives: Walk through specific examples
30‚Äì45 mins	Root Cause Analysis: Why was it a false positive?
45‚Äì55 mins	Improvement Actions: How to prevent or suppress similar future findings
55‚Äì60 mins	Wrap-up & Action Items
üß† Topics to Cover

    Summary of Total Findings vs. Valid vs. False Positives

    Breakdown by Tool (e.g., SAST vs. Dependency Scanner)

    Recurring Patterns (e.g., false positives due to specific libraries, frameworks, custom code)

    Examples: Concrete samples from recent scans

    Team Feedback on tooling experience, false positive management, and potential gaps

    Rule Tuning and Suppressions policies

üîß Actions to Take After the Meeting
üîÑ Process & Workflow Improvements

    Update Suppression Lists or Filters in your security tools.

    Tune or Reconfigure Rules causing noisy results.

    Adjust Code Patterns (where applicable) to avoid misidentification by tools.

üìò Documentation

    Maintain a False Positives Knowledge Base (internal wiki or shared document).

        Description

        Tool & Rule ID

        Reason it‚Äôs a false positive

        Suppression method (if any)

        Owner

üß™ Test & Validation

    Run test scans after tuning rules to verify effectiveness.

üìä Metrics to Track

    % of findings identified as false positives

    Time to resolution for true vs. false positives

    Tool-specific false positive rate

    Developer satisfaction/feedback scores (optional)

üß© Tips for Long-Term Success

    Assign Ownership: Make someone accountable for follow-ups (e.g., DevSecOps lead).

    Use Tags/Labels in Issue Tracker to categorize false positives.

    Incorporate into Sprint Retros for faster feedback loops.

    Train Developers on common secure coding false positives.

